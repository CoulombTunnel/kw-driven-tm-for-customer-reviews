---
title: "BA_fin"
author: "Markus Weiss"
date: "2024-03-10"
output:
  html_notebook: default
  pdf_document: default
  html_document:
    df_print: paged
---

```{r echo = F, message = F, include = F}
options(tinytex.verbose = TRUE)
#if (!require("rpart")) install.packages("rpart"); library("rpart")
#if (!require("rpart.plot")) install.packages("rpart.plot"); library("rpart.plot")
#if (!require("car")) install.packages("car"); library("car")
#if (!require("caret")) install.packages("caret"); library("caret")
if (!require("knitr")) install.packages("knitr"); library("knitr")
if (!require("stringr")) install.packages("stringr"); library("stringr")
if (!require("dplyr")) install.packages("dplyr"); library("dplyr")
if (!require("ggplot2")) install.packages("ggplot2"); library("ggplot2")
#if (!require("reshape2")) install.packages("reshape2"); library("reshape2")
if (!require("ggpubr")) install.packages("ggpubr"); library("ggpubr")
if (!require("corrplot")) install.packages("corrplot"); library("corrplot")
#if (!require("recipes")) install.packages("recipes"); library("recipes")
if (!require("tidymodels")) install.packages("tidymodels"); library("tidymodels")
if (!require("lubridate")) install.packages("lubridate"); library("lubridate")
#if (!require("readxl")) install.packages("readxl"); library("readxl")
#if (!require("janitor")) install.packages("janitor"); library("janitor")
if (!require("here")) install.packages("here"); library("here")
#if (!require("skimr")) install.packages("skimr"); library("skimr")
if (!require("Tmisc")) install.packages("Tmisc"); library("Tmisc")
#if (!require("hexbin")) install.packages("hexbin"); library("hexbin")
if (!require("plotly")) install.packages("plotly"); library("plotly")
#if (!require("gapminder")) install.packages("gapminder"); library("gapminder")
if (!require("stargazer")) install.packages("stargazer"); library("stargazer")
#if (!require("DataExplorer")) install.packages("DataExplorer"); library("DataExplorer")
if (!require("psych")) install.packages("psych"); library("psych")
if (!require("pastecs")) install.packages("pastecs"); library("pastecs")
if (!require("jsonlite")) install.packages("jsonlite"); library("jsonlite")
if (!require("tidyr")) install.packages("tidyr"); library("tidyr")
if (!require("quanteda")) install.packages("quanteda"); library("quanteda")
if (!require("quanteda.textstats")) install.packages("quanteda.textstats"); library("quanteda.textstats")
if (!require("quanteda.textplots")) install.packages("quanteda.textplots"); library("quanteda.textplots")
if (!require("wordcloud")) install.packages("wordcloud"); library("wordcloud")
if (!require("text2vec")) install.packages("text2vec"); library("text2vec")
if (!require("textmineR")) install.packages("textmineR"); library("textmineR")
#if (!require("topicmodels")) install.packages("topicmodels"); library("topicmodels")
#if (!require("tm")) install.packages("tm"); library("tm")
#if (!require("Matrix")) install.packages("Matrix"); library("Matrix")
if (!require("seededlda")) install.packages("seededlda"); library("seededlda")
if (!require("keyATM")) install.packages("keyATM"); library("keyATM")
if (!require("anytime")) install.packages("anytime"); library("anytime")
if (!require("data.table")) install.packages("data.table"); library("data.table")
if (!require("readr")) install.packages("readr"); library("readr")
if (!require("plyr")) install.packages("plyr"); library("plyr")
#if (!require("kernlab")) install.packages("kernlab"); library("kernlab")
if (!require("e1071")) install.packages("e1071"); library("e1071")
#if (!require("parallel")) install.packages("parallel"); library("parallel")
if (!require("foreach")) install.packages("foreach"); library("foreach")
if (!require("doParallel")) install.packages("doParallel"); library("doParallel")
if (!require("xtable")) install.packages("xtable"); library("xtable")

# cleaning the working environment
rm(list = ls())
# setting seed for reproducibility
set.seed(69)
knitr::opts_chunk$set(message = F, fig.width = 8, fig.height = 4)

```

```{r message = F}
# Function to load, randomly sample, process and tokenize the review files in batches
process_reviews <- function(data_dir, files, batch_size, num_rows,file_ending) {
  num_batches <- ceiling(length(files) / batch_size)
  for (batch in 1:num_batches) {
    start_index <- (batch - 1) * batch_size + 1
    end_index <- min(batch * batch_size, length(files))
    batch_files <- files[start_index:end_index]
    
    processed_data_list <- lapply(batch_files, function(file_name) {
      full_file_path <- file.path(data_dir, file_name)
      product_category <- gsub("(_5\\.json\\.gz)$", "", file_name) # Extract product category
      product_category <- gsub("-", " ", product_category) # Format product category
      
      # Process the gzipped JSON file
      processed_data <- readLines(gzfile(full_file_path), n = 500000)
      processed_data <- jsonlite::stream_in(textConnection(
        gsub("\\n", "", processed_data))) %>%
        jsonlite::flatten() %>%
        mutate(across(where(is.list), as.character)) %>%
        select(-contains("style")) %>%
        filter(verified == "TRUE") %>%
        select(-c(image, verified, unixReviewTime)) %>%
        unite(text, c(summary, reviewText), sep = " ") %>%
        mutate(text_length = nchar(text),
               reviewTime = as.Date(reviewTime, format="%m %d, %Y"),
               Product_Category = product_category) %>%
        distinct(overall, reviewTime, reviewerID, asin, Product_Category,
                 .keep_all = TRUE)
               #mutate(across(where(is.character), ~ gsub('"', '""', .))), # Escape quotes
               #mutate(across(where(is.character), ~ gsub("\n", " ", .))), # Replace newline characters with spaces
               #mutate(across(where(is.character), ~ paste0('"', ., '"')))) # Ensure fields are quoted
      
      # Reduce processed_data to num_rows random rows
      if(nrow(processed_data) > num_rows) {
        processed_data <- processed_data %>% sample_n(num_rows,
                                                      replace = FALSE)
      }
      
      processed_data
    })
    
    batch_data <- rbindlist(processed_data_list)
    rm(processed_data_list)
    
    # Create a unique identifier
    batch_data$doc_id <- paste(batch_data$asin, batch_data$reviewerID,
                               batch_data$reviewTime, batch_data$overall,
                               batch_data$Product_Category, sep = "_")
    
    # Create a corpus
    corp <- corpus(batch_data$text, docnames = batch_data$doc_id)
    rm(batch_data)
    
    # Perform preprocessing steps
    toks <- tokens(corp, remove_punct = TRUE, remove_numbers = TRUE,
                   remove_symbols = TRUE) %>%
    tokens_remove(stopwords("en")) %>%
    tokens_wordstem(language = "english")
    rm(corp)
    
    # Save the tokens as an RDS file for later use in R
    saveRDS(toks, file.path(gsub("(_5\\.json\\.gz)$",
                                 paste0("_toks_", file_ending, ".rds"),
                                 batch_files)))
    
    # Force garbage collection
    #rm(toks)
    gc(verbose = FALSE)
  
  }
}

# Datasets
data_dir <- "data/"
#files <- c("All_Beauty_5.json.gz", "AMAZON_FASHION_5.json.gz", "Appliances_5.json.gz", 
           #"Arts_Crafts_and_Sewing_5.json.gz", "Automotive_5.json.gz", 
           #"CDs_and_Vinyl_5.json.gz", "Cell_Phones_and_Accessories_5.json.gz", 
           #"Digital_Music_5.json.gz", 
           #"Electronics_5.json.gz", "Gift_Cards_5.json.gz", "Grocery_and_Gourmet_Food_5.json.gz", 
           #"Home_and_Kitchen_5.json.gz", "Industrial_and_Scientific_5.json.gz", 
           #"Kindle_Store_5.json.gz", "Luxury_Beauty_5.json.gz", "Magazine_Subscriptions_5.json.gz", 
           #"Movies_and_TV_5.json.gz", "Musical_Instruments_5.json.gz", "Office_Products_5.json.gz", 
           #"Patio_Lawn_and_Garden_5.json.gz", "Pet_Supplies_5.json.gz", "Prime_Pantry_5.json.gz", 
           #"Software_5.json.gz", "Sports_and_Outdoors_5.json.gz", "Tools_and_Home_Improvement_5.json.gz", 
           #"Toys_and_Games_5.json.gz", "Video_Games_5.json.gz")

files <- list.files("data/", pattern = "_5\\.json.gz$",
                    full.names = FALSE)

batch_size <- 1  # Define the number of files to process in each batch
#num_rows <- 2500
#file_ending <- "2k5"

# Process the review files for 1k and 2k5 rows per category respectively
process_reviews(data_dir, files, batch_size, 1000, "1k")
process_reviews(data_dir, files, batch_size, 2500, "2k5")

```

```{r echo = F, message = F, include = F}
start_time <- Sys.time()  # Record the start time

```

```{r message = F}
# Function to load and process token files
load_and_process_tokens <- function(directory, file_pattern) {
  # List all files in the directory with the given pattern
  toks_files <- list.files(directory, pattern = file_pattern,
                           full.names = TRUE)
  
  # Initialize an empty list to store loaded tokens
  toks_list <- list()
  
  # Loop through each file, load the tokens object, and add to the list
  for (f in toks_files) {
    toks_list[[length(toks_list) + 1]] <- readRDS(f)
  }
  
  # Convert list of tokens objects to a single tokens object
  toks <- do.call(c, toks_list)
  #toks <- tokens(do.call(c, toks))
  #toks <- do.call(quanteda::tokens, c(list(x = toks), list(what = "fastest")))

  # Create a document-feature matrix
  toks_dfm <- quanteda::dfm(toks)

  # Print the dimensions of the dfm
  print(dim(toks_dfm))
  
  # Print the first few rows and columns of the document-term matrix
  print(toks_dfm)

  # Return the dfm for further inspection or processing
  return(toks_dfm)
}

# Process tokens
toks_1k_dfm <- load_and_process_tokens("tokens/", "1k\\.rds$")
toks_2k5_dfm <- load_and_process_tokens("tokens/", "2k5\\.rds$")

```

```{r message = F}
dfm_tfidf(toks_1k_dfm, scheme_tf = "count")
dfm_weight(toks_1k_dfm)
summary(colSums(dfm_tfidf(toks_1k_dfm, scheme_tf = "count")))

dfm_tfidf(toks_2k5_dfm, scheme_tf = "count")
dfm_weight(toks_2k5_dfm)
summary(colSums(dfm_tfidf(toks_2k5_dfm, scheme_tf = "count")))

```

```{r message = F}
# Custom function to trim dfm based on tfidf threshold
dfm_trim_tfidf <- function(dfm, threshold) {
  # Calculate the TF-IDF scores
  tfidf <- dfm_tfidf(dfm, scheme_tf = "count", scheme_df = "inverse")
  
  # Identify terms that meet or exceed the threshold
  keep_terms <- featnames(tfidf)[colSums(tfidf >= threshold) > 0]
  
  # Select these terms in the dfm
  trimmed_dfm <- dfm_select(dfm, pattern = keep_terms, selection = "keep")
  
  return(trimmed_dfm)
}

# Function to remove documents with no terms after trimming
remove_empty_docs <- function(dfm) {
  # Subset the dfm to include only non-zero length documents
  non_empty_dfm <- dfm[which(rowSums(dfm) > 0), ]
  return(non_empty_dfm)
}

# Trim terms with tfidf below a threshold (here the median tfdif) and remove empty documents
tfidf_threshold_1k <- 8.2
tfidf_threshold_2k5 <- 4.8

toks_1k_dfm_filtered <- dfm_trim_tfidf(toks_1k_dfm, tfidf_threshold_1k)
toks_1k_dfm_filtered <- remove_empty_docs(toks_1k_dfm_filtered)

toks_2k5_dfm_filtered <- dfm_trim_tfidf(toks_2k5_dfm, tfidf_threshold_2k5)
toks_2k5_dfm_filtered <- remove_empty_docs(toks_2k5_dfm_filtered)

toks_1k_dfm
toks_1k_dfm_filtered
toks_2k5_dfm
toks_2k5_dfm_filtered

```

```{r message = F}
# Function to summarize a dfm and visualize top features
analyze_and_visualize_dfm <- function(dfm, num_top_features = 200, num_words_in_cloud = 100) {
  # Summary statistics for a DFM
  print(summary(dfm))
  top_terms <- topfeatures(dfm, 10)
  print(top_terms)
  doc_lengths <- textstat_summary(dfm, measure = "length")
  print(doc_lengths)
  sparsity <- 1 - as.numeric(nnzero(dfm)) / (nrow(dfm) * ncol(dfm))
  print(paste("Sparsity of DFM:", sparsity))
  
  # Extract and visualize top features from a dfm
  top_features <- topfeatures(dfm, n = num_top_features)
  print(top_features)
  
  word_freqs <- textstat_frequency(dfm_weight(dfm, scheme = "count"))
  top_words <- head(word_freqs, num_top_features)  # Select top words for clearer visualization
  
  # Create the word cloud
  wordcloud(words = top_words$feature, freq = top_words$frequency, 
            min.freq = 1,
            max.words = num_words_in_cloud, 
            random.order = FALSE,
            rot.per = 0.25,
            scale = c(4, 0.5),
            colors = brewer.pal(8, "Dark2"),)
}

# Analyze the dfm's
analyze_and_visualize_dfm(toks_1k_dfm_filtered)
analyze_and_visualize_dfm(toks_2k5_dfm_filtered)

```

```{r message = F}
# Function to calculate statistics for a single dfm
get_dfm_stats <- function(dfm) {
  c(
    ndoc(dfm),
    nfeat(dfm),
    sum(dfm),
    sparsity(dfm),
    mean(ntoken(dfm)),
    median(ntoken(dfm)),
    min(ntoken(dfm)),
    max(ntoken(dfm))
  )
}

# Calculate descriptive statistics for both dfm's
dfm_stats <- data.frame(
  Statistic = c(
    "Number of Documents",
    "Number of Features",
    "Total Tokens",
    "Sparsity",
    "Average Document Length",
    "Median Document Length",
    "Min Document Length",
    "Max Document Length"
  ),
  toks_1k_dfm_stats = get_dfm_stats(toks_1k_dfm_filtered),
  toks_2k5_dfm_stats = get_dfm_stats(toks_2k5_dfm_filtered)
)

# Convert the data frame to an xtable object
dfm_stats_xtable <- xtable(dfm_stats, 
            caption = "Descriptive Statistics of the DFM's", 
            label = "tab:dfm_stats_comparison",
            digits = c(0, 0, 3, 3))

# Save the table as a LaTeX file
print(dfm_stats_xtable, type = "latex", file = "dfm_stats_comparison.tex", 
      include.rownames = FALSE, 
      floating = TRUE, 
      table.placement = "h", 
      caption.placement = "top")

dfm_stats

```

```{r message = F}
# Define the range of topics, alpha, and beta
k_range <- c(seq(1, 26, by = 5), 29, seq(31, 100, by = 5))
alpha_range <- c(0.1, 0.5, 1)
beta_range <- c(0.01, 0.05, 0.1)

# Prepare a data frame to store the results
lda_cv_results <- data.frame(Topics = integer(),
                      Alpha = numeric(),
                      Beta = numeric(),
                      LogLikelihood = numeric(),
                      stringsAsFactors = FALSE)

# Loop over parameter ranges
for (k in k_range) {
  for (alpha in alpha_range) {
    for (beta in beta_range) {
      # Run LDA model
      lda_cv_model <- textmodel_lda(
        toks_2k5_dfm_filtered,
        k = k,
        alpha = alpha,
        beta = beta,
        max_iter = 100,
        auto_iter = FALSE,
        verbose = FALSE
      )
      
      # Get document-topic probabilities
      topic_prob <- lda_cv_model$theta

      # Calculate how well a document can be assigned to a topic
      fit <- sum(log(topic_prob[cbind(seq_len(nrow(topic_prob)),
                                      max.col(topic_prob))]))

      # Store the results
      lda_cv_results <- rbind(lda_cv_results, data.frame(
        Topics = k,
        Alpha = alpha,
        Beta = beta,
        Fit = fit
      ))
    }
  }
}

# print the results sorted by fit
print(lda_cv_results[order(-lda_cv_results$Fit), ])

```

```{r message = F}
# Run normal topic model with optimal k, alpha and beta from cross-validation
lda_model <- textmodel_lda(
  toks_2k5_dfm_filtered,
  k = 29,
  max_iter = 2000,
  auto_iter = FALSE,
  alpha = 0.1,
  beta = 0.1,
  gamma = 0,
  model = NULL,
  batch_size = 1,
  verbose = quanteda_options("verbose")
)

# Print the top words for each topic in the normal lda topic model
terms_lda <- terms(lda_model, 10)
print(terms_lda)

```

```{r message=FALSE}
#knitr::kable(terms_lda)
#print(xtable(terms_lda, type = "latex"), file = "filename2.tex")

```

```{r message = F}
# Dictionary holding topics with their seedwords
topics <- list(
AmazonFashion = c("amazon", "amaz","shoe", "comfort", "wear", "bag", "pair", "fit","feet", "thin", "bodi", "tight", "boot", "pocket", "size", "look"),
AllBeauty = c("hair", "smell", "skin", "brush", "shampoo", "face", "oil", "scent", "nail", "polish", "cream"),
Appliances = c("instal", "control", "unit", "switch", "machin", "button", "upgrad", "function", "appli", "mount", "cord", "plug"),
ArtsCraftsAndSewing = c("paint", "written", "tape", "thick", "screw", "tip", "metal", "block", "sharp", "knife", "ink", "hook"),
Automotive = c("car", "charg", "devic", "secur", "drive", "glass", "lock", "door", "key", "hose", "strap", "train"),
Books = c("book", "stori", "charact", "write", "author", "page", "written", "word", "inform", "begin", "creat", "end"),
CDsAndVinyl = c("song", "album", "music", "cd", "record", "track", "dvd", "player", "hear", "heard", "sound", "listen"),
CellPhonesAndAccessories = c("phone", "batteri", "cabl", "charg", "connect", "updat", "protect", "cord", "plug", "glass", "mount"),
ClothingShoesAndJewelry = c("shoe", "comfort", "wear", "bag", "pair", "feet", "boot", "pocket", "fit", "size", "look", "tight"),
DigitalMusic = c("music", "song", "album", "version", "listen", "download", "player", "track", "updat", "sound", "hear", "heared"),
Electronics = c("batteri", "control", "cabl", "screen", "charg", "devic", "switch", "connect", "button", "upgrad", "function"),
GiftCards = c("gift", "card", "christma", "idea", "present"),
GroceryAndGourmetFood = c("tast", "food", "flavor", "tea", "coffe","eat", "bottl", "mix", "cook", "oil", "recip", "snack", "spice", "fresh"),
HomeAndKitchen = c("water", "cook", "machin", "pan", "glass", "cup", "heat", "knife", "door", "coffe"),
IndustrialAndScientific = c("control", "unit", "cabl", "wire", "screw", "metal", "mount", "glass", "lock", "key"),
KindleStore = c("amazon", "amaz", "book", "version", "download", "author", "page", "read", "inform", "phone", "updat", "screen", "kindle", "stori", "charact", "write", "written"),
LuxuryBeauty = c("hair", "skin", "smell", "face", "oil", "scent", "polish", "cream", "treat", "brush", "shampoo", "nail"),
MagazineSubscriptions = c("magazin", "interest", "seri", "write", "inform", "articl", "page", "updat", "read", "written", "author"),
MoviesAndTV = c("movi", "watch", "seri", "film", "video", "dvd", "tv", "entertain", "stori", "charact", "end", "act"),
MusicalInstruments = c("music", "sound", "guitar", "string", "rock", "tune", "effect", "cord", "strap", "hook", "play"),
OfficeProducts = c("work", "paper", "write", "pen", "print", "tape", "ink", "file", "organiz", "calendar", "use"),
PatioLawnAndGarden = c("water", "leav", "feet", "saw", "blade", "paint", "hose", "hook", "mount", "outdoor", "natur"),
PetSupplies = c("dog", "hair", "smell", "eat", "food", "treat", "walk", "cat", "toy", "bath"),
PrimePantry = c("amazon", "amaz", "food", "tast", "flavor", "tea", "coffe", "bottl", "mix", "recip", "snack", "spice", "cook", "oil", "fresh"),
Software = c("softwar", "instal", "comput", "download", "updat", "file", "user", "program", "system", "pc", "onlin"),
SportsAndOutdoors = c("size", "water", "shoe", "sturdi", "feet", "ball", "walk", "inch", "tight", "summer", "outdoor", "bike", "comfort", "wear", "pair", "fit", "thin", "bodi", "boot", "pocket", "size", "look", "natur"),
ToolsAndHomeImprovement = c("tool", "saw", "blade", "screw", "metal", "tape", "knife", "lock", "key", "drill", "hammer", "nail", "sturdi", "quality", "use"),
ToysAndGames = c("game", "play", "fun", "kid", "toy", "charact", "collect", "young", "girl", "boy", "creat", "build", "puzzle"),
VideoGames = c("game", "play", "version", "control", "screen", "graphic", "level", "player", "system", "download", "action", "adventur", "difficult", "beat", "onlin")
)

#topics <- list(
#ProductQuality = c("sturdi", "durabl", "well-made", "high-quality", "premium", "long-last", "reli", "craft", "construction"),
#EaseOfUse = c("easy", "simple", "intuit", "user-friend", "straightforward", "effortless", "uncomplic", "beginn", "quick", "conveni"),
#Comfort = c("comfort", "soft", "plush", "cushion", "padded", "breath", "cozi", "relax", "sooth", "gentle"),
#Versatility = c("versatil", "multifunct", "adapt", "flexibl", "vari", "multipl", "divers", "all-in-one", "comprehen", "wide-rang"),
#Innovation = c("innov", "novel", "invent", "cutting-edg", "advanc", "state-of-the-art", "pioneer", "groundbreak", "revolution", "smart"),
#GiftIdeas = c("gift", "present", "surpris", "thoughtful", "person", "special", "occas", "celebr", "holiday", "birthda"),
#Entertainment = c("entertain", "fun", "enjoy", "excit", "thrill", "amus", "engag", "captiv", "fascin", "imaginative"),
#HealthAndWellness = c("health", "wellness", "well-b", "fit", "nutrit", "nourish", "wholesome", "clean", "pure", "organic"),
#Portability = c("portabl", "lightweight", "compact", "travel-friend", "on-the-go", "hand-held", "pocket-s", "mobile", "take-anywher"),
#Sustainability = c("sustain", "eco-friend", "green", "environment", "recycl", "biodegra", "renewabl", "ethic", "responsib", "conscious"),
#Organization = c("organ", "storage", "arrang", "tidi", "declut", "systemat", "order", "compartment", "sort", "categor"),
#Creativity = c("creativ", "imaginative", "inspir", "artist", "express", "origin", "invent", "maker", "diy", "craft"),
#Technology = c("tech", "electron", "digital", "comput", "program", "code", "automat", "robot", "ai", "smart"),
#LearningAndDevelopment = c("learn", "develop", "grow", "skill", "educat", "teach", "train", "practic", "improv", "progress"),
#Customization = c("custom", "personaliz", "tailor", "modifi", "adapt", "adjust", "tweak", "configur", "unique", "individual")
#)

#topics <- list(
#AmazonFashion = c("trend", "style", "chic", "elegant", "casual", "accessori", "outfit", "wardrob", "apparel", "designer"),
#AllBeauty = c("cosmetic", "makeup", "skincare", "fragranc", "groom", "pamper", "spa", "salon", "moistur", "nourish"),
#Appliances = c("kitchen", "home", "gadget", "refriger", "oven", "washer", "dryer", "vacu", "blender", "coffemak"),
#ArtsCraftsAndSewing = c("diy", "handmade", "quilt", "knit", "crochet", "embroider", "bead", "jewelri", "scrapbook", "stamp"),
#Automotive = c("vehicl", "tire", "engine", "brake", "oil", "fuel", "transmiss", "accessor", "gps", "car"),
#Books = c("novel", "fiction", "nonfict", "memoir", "biographi", "poetri", "cookbook", "children", "comic", "manga"),
#CDsAndVinyl = c("album", "artist", "band", "label", "genre", "soundtrack", "classic", "collector", "turntabl", "playlist"),
#CellPhonesAndAccessories = c("smartphone", "case", "screen", "protector", "charger", "headphon", "earbud", "bluetooth", "selfie", "tripod"),
#ClothingShoesAndJewelry = c("fashion", "trend", "style", "design", "casual", "formal", "vintag", "accessori", "footwear", "athlet"),
#DigitalMusic = c("stream", "download", "mp3", "playlist", "album", "artist", "genre", "radio", "podcast", "audiobook"),
#Electronics = c("gadget", "tech", "audio", "video", "gaming", "computer", "laptop", "tablet", "camera", "drone"),
#GiftCards = c("birthday", "holiday", "anniversari", "congratul", "thank", "appreci", "special", "thought", "last-minut", "e-gift"),
#GroceryAndGourmetFood = c("snack", "beverag", "pantri", "stapl", "organ", "gluten-fre", "vegan", "gourmet", "intern", "special"),
#HomeAndKitchen = c("decor", "furnitur", "bedding", "bath", "applianc", "cookwar", "utensil", "storag", "organ", "clean"),
#IndustrialAndScientific = c("lab", "medic", "dental", "safeti", "suppli", "equipment", "instrument", "tool", "manufactur", "construct"),
#KindleStore = c("ebook", "e-reader", "digital", "download", "bestseller", "author", "genre", "seri", "publish", "review"),
#LuxuryBeauty = c("premium", "high-end", "prestig", "designer", "spa", "salon", "treatment", "anti-ag", "serum", "mask"),
#MagazineSubscriptions = c("subscript", "issu", "print", "digital", "publish", "editor", "journalist", "articl", "stori", "interview"),
#MoviesAndTV = c("film", "cinema", "actor", "actress", "director", "producer", "documentari", "seri", "episode", "stream"),
#MusicalInstruments = c("guitar", "piano", "keyboard", "drum", "violin", "flute", "saxophone", "trumpet", "amplifi", "record"),
#OfficeProducts = c("suppli", "stationeri", "printer", "ink", "toner", "paper", "calendar", "organiz", "desk", "chair"),
#PatioLawnAndGarden = c("outdoor", "furnitur", "grill", "patio", "deck", "yard", "garden", "landscap", "plant", "flower"),
#PetSupplies = c("dog", "cat", "bird", "fish", "rodent", "reptil", "food", "treat", "toy", "groom"),
#PrimePantry = c("snack", "beverag", "breakfast", "bake", "clean", "paper", "household", "storag", "organ", "save"),
#Software = c("program", "app", "antivirus", "productiv", "business", "design", "video", "music", "photo", "game"),
#SportsAndOutdoors = c("athlet", "fitness", "exercis", "camp", "hike", "bike", "swim", "fish", "hunt", "golf"),
#ToolsAndHomeImprovement = c("diy", "repair", "renov", "construct", "paint", "electr", "plumb", "secur", "light", "hardwar"),
#ToysAndGames = c("play", "fun", "imagin", "creat", "puzzl", "board", "card", "video", "outdoor", "educat"),
#VideoGames = c("console", "pc", "nintendo", "xbox", "playstat", "action", "adventur", "rpg", "simulat", "sport")
#)

#topics <- list(
#  ProductQuality = c("quality", "well-made", "durable", "sturdy", "craftsmanship", "reliable", "long-lasting", "premium", "high-end", "luxurious", "flimsy", "cheap", "poorly made", "defective", "broke", "falling apart"),
#  CustomerService = c("service", "support", "help", "representative", "agent", "rep", "customer service", "responsive", "quick", "helpful", "friendly", "rude", "unhelpful", "slow", "unresponsive", "frustrating"),
#  ShippingAndDelivery = c("shipping", "delivery", "arrived", "package", "fast", "quick", "on-time", "early", "delay", "late", "slow", "lost", "damaged", "wrong item", "tracking", "packaging"),
#  ValueForMoney = c("value", "price", "worth", "affordable", "expensive", "cheap", "cost", "budget", "overpriced", "good deal", "great value", "waste of money", "ripoff", "not worth it"),
#  EaseOfUse = c("easy", "simple", "intuitive", "user-friendly", "straightforward", "complicated", "confusing", "difficult", "hard to use", "not user-friendly", "instructions", "manual", "guide", "setup", "installation"),
#  FeaturesAndFunctionality = c("features", "functionality", "functions", "capabilities", "versatile", "multi-purpose", "limited", "basic", "lacking features", "missing features", "bells and whistles", "advanced", "high-tech"),
#  Performance = c("performance", "performs", "powerful", "fast", "responsive", "smooth", "slow", "sluggish", "laggy", "glitchy", "crash", "freeze", "reliable", "consistent", "intermittent", "spotty"),
#  ComfortAndFit = c("comfort", "comfortable", "soft", "cozy", "plush", "breathable", "itchy", "scratchy", "rough", "uncomfortable", "fits", "true to size", "runs small", "runs large", "too tight", "too loose", "stretchy"),
#  AppearanceAndStyle = c("appearance", "style", "stylish", "fashionable", "trendy", "cute", "attractive", "ugly", "unflattering", "dated", "looks", "color", "pattern", "design", "aesthetic", "finish", "material"),
#  DurabilityAndBuildQuality = c("durable", "long-lasting", "sturdy", "well-built", "solid", "heavy-duty", "rugged", "flimsy", "delicate", "fragile", "cheaply made", "poor quality", "broke", "fell apart", "worn out"),
#  TasteAndFlavor = c("taste", "flavor", "delicious", "yummy", "tasty", "savory", "sweet", "salty", "bitter", "sour", "bland", "flavorless", "appetizing", "unappetizing", "fresh", "stale", "expired", "spoiled"),
#  Effectiveness = c("effective", "works", "results", "noticeable", "visible", "potent", "strong", "weak", "ineffective", "useless", "no results", "miracle", "holy grail", "game changer", "disappointing"),
#  EntertainmentAndEnjoyment = c("entertaining", "enjoyable", "fun", "engaging", "interesting", "boring", "dull", "slow", "exciting", "thrilling", "suspenseful", "funny", "hilarious", "laugh", "cry", "emotional", "moving"),
#  EducationalValue = c("educational", "informative", "insightful", "enlightening", "useful", "helpful", "practical", "comprehensive", "in-depth", "shallow", "basic", "lacking depth", "repetitive", "redundant"),
#  CompatibilityAndConnectivity = c("compatible", "connectivity", "connects", "syncs", "integrates", "works with", "doesn't work with", "incompatible", "connection issues", "pairing", "bluetooth", "wifi", "wired", "wireless"))

#topics <- list(
  #"Product Quality" = c("great", "good", "product", "qualiti", "well", "better", "high", "expect", "fine", "excel", "excellent", "awesom", "wonder", "bad", "poor", "disappoint", "cheap", "broke", "defect"),
  #"Customer Satisfaction" = c("love", "like", "just", "realli", "perfect", "best", "favorit", "enjoy", "happi", "thank", "glad", "impress", "amaz", "pleas", "satisfi", "hate", "regret", "return", "refund"),
  #"Ease of Use" = c("use", "easi", "work", "tri", "simpl", "straightforward", "intuit", "quick", "fast", "smooth", "difficult", "hard", "complic", "frustrat", "struggl", "confus"),
  #"Price and Value" = c("price", "worth", "money", "valu", "afford", "bargain", "deal", "expens", "overpric", "cheap", "cost", "budget", "save", "wast"),
  #"Shipping and Delivery" = c("ship", "deliveri", "arriv", "fast", "quick", "delay", "slow", "late", "damag", "lost", "wrong", "track", "packag"),
  #"Product Features" = c("size", "color", "fit", "materi", "fabric", "style", "design", "pattern", "look", "feel", "comfort", "durabl", "sturdi", "light", "heavyduty"),
  #"Customer Service" = c("servic", "help", "support", "repres", "agent", "contact", "respons", "friendli", "rude", "unhelpful", "slow", "frustrat", "terribl"),
  #"Gift and Occasion" = c("gift", "occas", "birthday", "holiday", "christma", "anniversari", "special", "celebr", "parti", "present"),
  #"Entertainment" = c("book", "game", "movi", "stori", "song", "music", "album", "charact", "entertain", "fun", "enjoy", "interest", "excit", "bore", "dull"),
  #"Recommendation" = c("recommend", "buy", "purchas", "try", "suggest", "advis", "great", "good", "love", "like", "best", "favorit", "excel", "awesom", "wonder"))

dict <- dictionary(topics)

seededlda_model <- textmodel_seededlda(toks_2k5_dfm_filtered, dict, residual = TRUE, min_termfreq = 10,
max_iter = 1500)

seededlda_terms <- seededlda::terms(seededlda_model)
knitr::kable(seededlda_terms)

# Create the term co-occurrence matrix (TCM)
tcm <- quanteda::fcm(toks_2k5_dfm_filtered, context = "document", count = "frequency", tri = FALSE)

# Calculate the normalized pointwise mutual information coherence
seededlda_coherence_scores <- text2vec::coherence(seededlda_terms, tcm,
                                                  metrics = "mean_npmi",
                                                  n_doc_tcm = nrow(
                                                    toks_2k5_dfm_filtered))
print(seededlda_coherence_scores)

```

```{r message = F}
# List  holding topics with their keywords
keywords <- topics

docs <- keyATM_read(toks_2k5_dfm_filtered)

keyATM_model <- keyATM(
  docs = docs,
  model = "base",
  no_keyword_topics = 1,
  keywords = keywords,
  options = list(seed = 69, iterations = 1500)
)

# Print the top terms for each topic in the keyATM model
keyATM_terms <- top_words(keyATM_model, n = 10)
print(keyATM_terms)

# Show Log Likelihood (should increase) and Perplexity (should decrease)
plot_modelfit(keyATM_model)

# Show Alpha (should stabilize)
plot_alpha(keyATM_model)

# Show Pi
plot_pi(keyATM_model)

# Equation 1 of Mimno et al. 2011 adopted to keyATM.
#semantic_coherence(keyATM_model, docs, n = 10)

# Calculate the normalized pointwise mutual information coherence
keyATM_coherence_scores <- text2vec::coherence(as.matrix(top_words(keyATM_model, n = 10, show_keyword = FALSE)), tcm, metrics = "mean_npmi", n_doc_tcm = nrow(toks_2k5_dfm_filtered))
print(keyATM_coherence_scores)

```

```{r message = F}
# Function to perform classification with the topic model outputs and evaluate the results
classify_and_evaluate_model <- function(model, toks, ref_model = NULL) {
  # Extract the document-topic distribution (theta) from the model
  doc_topic_distr <- model$theta

  # Adjust column names to match a reference model, if provided
  if (!is.null(ref_model)) {
    ref_colnames <- colnames(ref_model$theta)
    colnames(doc_topic_distr) <- c(ref_colnames[1:29])
  }

  # Labels for the topics
  labels <- sapply(strsplit(toks_2k5_dfm_filtered@docvars$docname_, "_"),
                   function(x) paste(x[5:length(x)], collapse = ""))

  # Create a data frame for the classification task
  classification_data <- data.frame(doc_topic_distr)
  classification_data$label <- as.factor(labels)

  # Assign each observation to the topic with the maximum theta value
  predicted_topic_index <- apply(doc_topic_distr, 1, which.max)
  predicted_topic <- colnames(doc_topic_distr)[predicted_topic_index]

  # Remove everything before the first underscore and normalize names
  normalize_topic_names <- function(topic_name) {
    topic_name <- sub("^[0-9]+_", "", topic_name)  # Remove prefix numbers
    topic_name <- gsub("And", "and", topic_name)  # Normalize "And" to "and"
    topic_name <- sub("_[0-9]+$", "", topic_name) # Remove suffix numbers
    topic_name <- gsub("AmazonFashion", "AMAZONFASHION", topic_name)  # Specific normalization
    topic_name <- gsub("Other", "other", topic_name)  # Specific normalization
    return(topic_name)
  }

  predicted_topic <- sapply(predicted_topic, normalize_topic_names)
  classification_data$predicted_topic <- as.factor(predicted_topic)

  # Normalize column names in doc_topic_distr
  normalized_colnames <- sapply(colnames(doc_topic_distr), function(x) {
    normalize_topic_names(sub("^[0-9]+_", "", x))  # Remove prefix numbers and normalize
  })
  colnames(doc_topic_distr) <- normalized_colnames

  # Ensure both factors have the same levels
  all_levels <- union(levels(classification_data$label),
                      levels(classification_data$predicted_topic))
  classification_data$label <- factor(classification_data$label,
                                      levels = all_levels)
  classification_data$predicted_topic <- factor(
    classification_data$predicted_topic,
    levels = all_levels)

  # Determine overall accuracy
  overall_accuracy <- mean(
    classification_data$label == classification_data$predicted_topic)
  print(paste("Overall Accuracy: ", overall_accuracy))

  # Assess the theta values for each of the topics corresponding to the true label
  true_topic_theta_values <- sapply(1:nrow(classification_data),
                                    function(i) {
    label_index <- which(
      colnames(doc_topic_distr) ==
        as.character(classification_data$label[i]))
    if (length(label_index) > 0) {
      doc_topic_distr[i, label_index]
    } else {
      NA  # If no matching label index is found, return NA
    }
  })

  classification_data$true_topic_theta <- true_topic_theta_values

  # Calculate accuracy per category/label
  accuracy_per_category <- sapply(levels(classification_data$label),
                                  function(l) {
    category_data <- classification_data[classification_data$label == l, ]
    mean(category_data$label == category_data$predicted_topic,
         na.rm = TRUE)
  })

  accuracy_per_category_df <- data.frame(Category =
                                           names(accuracy_per_category),
                                         Accuracy =
                                           accuracy_per_category)
  print(accuracy_per_category_df[order(-accuracy_per_category_df$Accuracy),
                                 ])
  print(head(classification_data))
  
  # Cross-tabulation and heatmap visualization
  cross_tab <- table(classification_data$label,
                     classification_data$predicted_topic)
  cond_rel_freq <- prop.table(cross_tab, 1)

  # Convert the cross_tab to a long format using tidyr
  melted_cross_tab <- as.data.frame(as.table(as.matrix(cross_tab)))
  colnames(melted_cross_tab) <- c("True_Label", "Predicted_Label", "Value")
  
  # Create the heatmap
  heatmap <- ggplot(melted_cross_tab, aes(x = Predicted_Label, y = True_Label,
                                          fill = Value)) +
    geom_tile() +
    scale_fill_gradientn(colors = heat.colors(256), 
                         name = "Count") +
    theme_minimal() +
    theme(axis.text.x = element_text(angle = 45, hjust = 1),
          axis.text.y = element_text(size = 8),
          axis.title = element_text(face = "bold"),
          plot.title = element_text(hjust = 0.5)) +
    labs(x = "Predicted Label", y = "True Label") +
    coord_fixed()
  
  print(heatmap)
  
  # Save the plot
  #ggsave("keyATM_heatmap.png", width = 12, height = 10, dpi = 300)

  return(list(overall_accuracy = overall_accuracy,
              accuracy_per_category = accuracy_per_category_df,
              classification_data = classification_data))
}

# Use output of topic models for classification
lda_results <- classify_and_evaluate_model(lda_model,
                                           toks_2k5_dfm_filtered,
                                           seededlda_model)
seededlda_results <- classify_and_evaluate_model(seededlda_model,
                                                 toks_2k5_dfm_filtered)
keyATM_results <- classify_and_evaluate_model(keyATM_model,
                                              toks_2k5_dfm_filtered)

# Combine the overall accuracy results
overall_accuracy_df <- data.frame(
  Model = c("LDA", "SeededLDA", "keyATM"),
  Accuracy = c(lda_results$overall_accuracy,
               seededlda_results$overall_accuracy,
               keyATM_results$overall_accuracy)
)

# Combine the per category accuracy results
lda_accuracy <- lda_results$accuracy_per_category
lda_accuracy$Model <- "LDA"

seededlda_accuracy <- seededlda_results$accuracy_per_category
seededlda_accuracy$Model <- "SeededLDA"

keyATM_accuracy <- keyATM_results$accuracy_per_category
keyATM_accuracy$Model <- "keyATM"

per_category_accuracy_df <- rbind(lda_accuracy,
                                  seededlda_accuracy,
                                  keyATM_accuracy)

# Plot overall accuracy
ggplot(overall_accuracy_df, aes(x = Model, y = Accuracy, fill = Model)) +
  geom_bar(stat = "identity", position = position_dodge()) +
  theme_minimal() +
  labs(title = "Overall Accuracy Comparison", y = "Accuracy", x = "Model")

# Plot per category accuracy
ggplot(per_category_accuracy_df, aes(x = reorder(Category, -Accuracy), y = Accuracy,
                                     fill = Model)) +
  geom_bar(stat = "identity", position = position_dodge()) +
  theme_minimal() +
  labs(title = "Per Category Accuracy Comparison", y = "Accuracy", x = "Category") +
  theme(axis.text.x = element_text(angle = 90, hjust = 1))

```

```{r message = F}
# Demonstrate importance of topic definition with worse seed-/keywords
topics_bad <- list(
AmazonFashion = c("trend", "style", "chic", "elegant", "casual", "accessori", "outfit", "wardrob", "apparel", "designer"),
AllBeauty = c("cosmetic", "makeup", "skincare", "fragranc", "groom", "pamper", "spa", "salon", "moistur", "nourish"),
Appliances = c("kitchen", "home", "gadget", "refriger", "oven", "washer", "dryer", "vacu", "blender", "coffemak"),
ArtsCraftsAndSewing = c("diy", "handmade", "quilt", "knit", "crochet", "embroider", "bead", "jewelri", "scrapbook", "stamp"),
Automotive = c("vehicl", "tire", "engine", "brake", "oil", "fuel", "transmiss", "accessor", "gps", "car"),
Books = c("novel", "fiction", "nonfict", "memoir", "biographi", "poetri", "cookbook", "children", "comic", "manga"),
CDsAndVinyl = c("album", "artist", "band", "label", "genre", "soundtrack", "classic", "collector", "turntabl", "playlist"),
CellPhonesAndAccessories = c("smartphone", "case", "screen", "protector", "charger", "headphon", "earbud", "bluetooth", "selfie", "tripod"),
ClothingShoesAndJewelry = c("fashion", "trend", "style", "design", "casual", "formal", "vintag", "accessori", "footwear", "athlet"),
DigitalMusic = c("stream", "download", "mp3", "playlist", "album", "artist", "genre", "radio", "podcast", "audiobook"),
Electronics = c("gadget", "tech", "audio", "video", "gaming", "computer", "laptop", "tablet", "camera", "drone"),
GiftCards = c("birthday", "holiday", "anniversari", "congratul", "thank", "appreci", "special", "thought", "last-minut", "e-gift"),
GroceryAndGourmetFood = c("snack", "beverag", "pantri", "stapl", "organ", "gluten-fre", "vegan", "gourmet", "intern", "special"),
HomeAndKitchen = c("decor", "furnitur", "bedding", "bath", "applianc", "cookwar", "utensil", "storag", "organ", "clean"),
IndustrialAndScientific = c("lab", "medic", "dental", "safeti", "suppli", "equipment", "instrument", "tool", "manufactur", "construct"),
KindleStore = c("ebook", "e-reader", "digital", "download", "bestseller", "author", "genre", "seri", "publish", "review"),
LuxuryBeauty = c("premium", "high-end", "prestig", "designer", "spa", "salon", "treatment", "anti-ag", "serum", "mask"),
MagazineSubscriptions = c("subscript", "issu", "print", "digital", "publish", "editor", "journalist", "articl", "stori", "interview"),
MoviesAndTV = c("film", "cinema", "actor", "actress", "director", "producer", "documentari", "seri", "episode", "stream"),
MusicalInstruments = c("guitar", "piano", "keyboard", "drum", "violin", "flute", "saxophone", "trumpet", "amplifi", "record"),
OfficeProducts = c("suppli", "stationeri", "printer", "ink", "toner", "paper", "calendar", "organiz", "desk", "chair"),
PatioLawnAndGarden = c("outdoor", "furnitur", "grill", "patio", "deck", "yard", "garden", "landscap", "plant", "flower"),
PetSupplies = c("dog", "cat", "bird", "fish", "rodent", "reptil", "food", "treat", "toy", "groom"),
PrimePantry = c("snack", "beverag", "breakfast", "bake", "clean", "paper", "household", "storag", "organ", "save"),
Software = c("program", "app", "antivirus", "productiv", "business", "design", "video", "music", "photo", "game"),
SportsAndOutdoors = c("athlet", "fitness", "exercis", "camp", "hike", "bike", "swim", "fish", "hunt", "golf"),
ToolsAndHomeImprovement = c("diy", "repair", "renov", "construct", "paint", "electr", "plumb", "secur", "light", "hardwar"),
ToysAndGames = c("play", "fun", "imagin", "creat", "puzzl", "board", "card", "video", "outdoor", "educat"),
VideoGames = c("console", "pc", "nintendo", "xbox", "playstat", "action", "adventur", "rpg", "simulat", "sport")
)

dict_bad <- dictionary(topics_bad)

seededlda_model_bad <- textmodel_seededlda(toks_2k5_dfm_filtered, dict_bad, residual = TRUE, min_termfreq = 10,
max_iter = 1500)

seededlda_terms_bad <- seededlda::terms(seededlda_model)
knitr::kable(seededlda_terms_bad)

# Calculate the normalized pointwise mutual information coherence
seededlda_coherence_scores_bad <- text2vec::coherence(seededlda_terms_bad, tcm, metrics = "mean_npmi", n_doc_tcm = nrow(toks_2k5_dfm_filtered))
print(seededlda_coherence_scores)

# List  holding topics with their keywords
keywords_bad <- topics_bad

keyATM_model_bad <- keyATM(
  docs = docs,
  model = "base",
  no_keyword_topics = 1,
  keywords = keywords_bad,
  options = list(seed = 69, iterations = 1500)
)

# Print the top terms for each topic in the keyATM model
keyATM_terms_bad <- top_words(keyATM_model_bad, n = 10)
print(keyATM_terms_bad)

# Show Log Likelihood (should increase) and Perplexity (should decrease)
plot_modelfit(keyATM_model_bad)

# Show Alpha (should stabilize)
plot_alpha(keyATM_model_bad)

# Show Pi
plot_pi(keyATM_model_bad)

# Calculate the normalized pointwise mutual information coherence
keyATM_coherence_scores_bad <- text2vec::coherence(as.matrix(top_words(keyATM_model_bad, n = 10, show_keyword = FALSE)), tcm, metrics = "mean_npmi", n_doc_tcm = nrow(toks_2k5_dfm_filtered))
print(keyATM_coherence_scores_bad)

# Use output of topic models for classification
seededlda_results_bad <- classify_and_evaluate_model(seededlda_model_bad,
                                                     toks_2k5_dfm_filtered)
keyATM_results_bad <- classify_and_evaluate_model(keyATM_model_bad,
                                                  toks_2k5_dfm_filtered)

# Combine the overall accuracy results
overall_accuracy_df_bad <- data.frame(
  Model = c("LDA", "SeededLDA_bad", "keyATM_bad"),
  Accuracy = c(lda_results$overall_accuracy,
               seededlda_results_bad$overall_accuracy,
               keyATM_results_bad$overall_accuracy)
)

# Combine the per category accuracy results
seededlda_accuracy_bad <- seededlda_results_bad$accuracy_per_category
seededlda_accuracy_bad$Model <- "SeededLDA_bad"

keyATM_accuracy_bad <- keyATM_results_bad$accuracy_per_category
keyATM_accuracy_bad$Model <- "keyATM_bad"

per_category_accuracy_df_bad <- rbind(lda_accuracy,
                                      seededlda_accuracy_bad,
                                      keyATM_accuracy_bad)

# Plot overall accuracy
ggplot(overall_accuracy_df_bad, aes(x = Model, y = Accuracy,
                                    fill = Model)) +
  geom_bar(stat = "identity", position = position_dodge()) +
  theme_minimal() +
  labs(title = "Overall Accuracy Comparison", y = "Accuracy", x = "Model")

# Plot per category accuracy
ggplot(per_category_accuracy_df_bad, aes(x = reorder(Category, -Accuracy),
                                         y = Accuracy,
                                         fill = Model)) +
  geom_bar(stat = "identity", position = position_dodge()) +
  theme_minimal() +
  labs(y = "Accuracy", x = "Category",
       title = "Per Category Accuracy Comparison") +
  theme(axis.text.x = element_text(angle = 90, hjust = 1))

```

```{r echo = F, message = F, include = F}
end_time <- Sys.time()  # Record the end time
execution_time <- end_time - start_time  # Calculate the execution time

print(execution_time)  # Print the execution time

```

Time difference of 4.076687 hours


